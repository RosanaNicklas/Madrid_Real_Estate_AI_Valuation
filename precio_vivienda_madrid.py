# -*- coding: utf-8 -*-
"""Precio_Vivienda_Madrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/RosanaNicklas/Precios_Vivienda_Madrid/blob/main/Precio_Vivienda_Madrid.ipynb
"""

from time import time
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler , StandardScaler, LabelEncoder
from sklearn import datasets, linear_model
from sklearn import preprocessing
from sklearn.preprocessing import Normalizer , scale
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesRegressor, VotingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

pisos = pd.read_csv('houses_Madrid.csv')

"""# DATASET

***El dataset contiene información sobre los precios de vivienda en Madrid, y la mayoría de las propiedades registradas son de tipo "pisos". Esto indica que la mayoría de las unidades de vivienda en el conjunto de datos son apartamentos ubicados en edificios de varios pisos. Estos pisos pueden variar en tamaño, ubicación, número de habitaciones y otras características que influyen en sus precios.***

***El hecho de que la mayoría de las propiedades sean pisos puede deberse a diversas razones, como la alta densidad de población en las áreas urbanas, la preferencia de los residentes por vivir en apartamentos más compactos y cercanos a servicios y comodidades, y la disponibilidad de suelo limitada para nuevas construcciones en el centro de la ciudad.***
"""

print('Contenido y Forma de los datos')
print('filas y columnas: {}'.format(pisos.shape))

"""# En este proyecto voy a construir un modelo sobre el  precio de la vivienda en Madrid.

***Partimos de un conjunto de datos que consta de 21742 filas y 58 columnas con diferentes
métricas como precio de vivienda, numero de habitaciones, numero de baños, metros cuadrados...***
"""

pisos.head()

"""# Primero vamos a eliminar la columna "Unnamed:0" y  vamos a tomar de index la columna "id"."""

pisos.drop(columns=['Unnamed: 0'], inplace=True)
pisos.set_index('id', inplace=True)
pisos.head()

print('filas y columnas: {}'.format(pisos.shape))
print('Total Entradas: {}'.format(pisos.size))

"""***El dataset que tienes contiene una serie de características (features) que describen diferentes aspectos de propiedades inmobiliarias. A continuación, te explicaré cada una de las características:***

1. 'título': El título del anuncio o publicación de la propiedad.

2. 'subtítulo': Un subtítulo adicional que complementa la descripción de la propiedad.

3. 'm2_construidos': La cantidad de metros cuadrados construidos de la propiedad.

4. 'm2_útiles': La cantidad de metros cuadrados útiles de la propiedad (área habitable).

5. 'n_habitaciones': El número de habitaciones (dormitorios) que tiene la propiedad.

6. 'n_baños': El número de baños que tiene la propiedad.

7. 'n_pisos': El número de pisos (plantas) que tiene la propiedad.

8. 'm2_asignación': La cantidad de metros cuadrados asignados a la propiedad, que podría referirse a áreas comunes en un edificio.

9. 'latitud': La coordenada de latitud geográfica de la ubicación de la propiedad.

10. 'longitud': La coordenada de longitud geográfica de la ubicación de la propiedad.

11. 'raw_address': La dirección sin procesar de la propiedad.

12. 'is_exact_address_hidden': Un valor booleano que indica si la dirección exacta de la propiedad está oculta o no.

13. 'street_name': El nombre de la calle en la que se encuentra la propiedad.

14. 'calle_número': El número de la calle donde se encuentra la propiedad.

15. 'portal': Número de portal o entrada a la propiedad (en edificios).

16. 'piso': El número de piso en el que se encuentra la propiedad.

17. 'es_piso_debajo': Un valor booleano que indica si la propiedad está en un piso debajo del nivel de la calle.

18. 'puerta': El número de puerta o apartamento en el que se encuentra la propiedad.

19. 'neighborhood_id': Identificador único del barrio o vecindario en el que se encuentra la propiedad.

20. 'operación': El tipo de operación, que podría ser "venta" o "alquiler".

21. 'rent_price': El precio de alquiler de la propiedad.

22. 'rent_price_by_area': El precio de alquiler dividido por el área útil de la propiedad.

23. 'is_rent_price_known': Un valor booleano que indica si el precio de alquiler es conocido o no.

24. 'buy_price': El precio de venta de la propiedad.

25. 'buy_price_by_area': El precio de venta dividido por el área útil de la propiedad.

26. 'is_buy_price_known': Un valor booleano que indica si el precio de venta es conocido o no.

27. 'house_type_id': Identificador único del tipo de propiedad (casa, piso, chalet, etc.).

28. 'is_renewal_needed': Un valor booleano que indica si la propiedad necesita renovación o no.

29. 'es_nuevo_desarrollo': Un valor booleano que indica si la propiedad es de nuevo desarrollo o no.

30. 'año_construido': El año de construcción de la propiedad.

31. 'tiene_calefacción_central': Un valor booleano que indica si la propiedad tiene calefacción central.

32. 'tiene_calefacción_individual': Un valor booleano que indica si la propiedad tiene calefacción individual.

33. 'se_admiten_mascotas': Un valor booleano que indica si se admiten mascotas en la propiedad.

34. 'tiene_ac': Un valor booleano que indica si la propiedad tiene aire acondicionado.

35. 'tiene_armarios_equipados': Un valor booleano que indica si la propiedad tiene armarios equipados.

36. 'tiene_ascensor': Un valor booleano que indica si la propiedad tiene ascensor.

37. 'es_exterior': Un valor booleano que indica si la propiedad es exterior.

38. 'tiene_jardín': Un valor booleano que indica si la propiedad tiene jardín.

39. 'tiene_piscina': Un valor booleano que indica si la propiedad tiene piscina.

40. 'tiene_terraza': Un valor booleano que indica si la propiedad tiene terraza.

41. 'tiene_balcón': Un valor booleano que indica si la propiedad tiene balcón.

42. 'tiene_trastero': Un valor booleano que indica si la propiedad tiene trastero.

43. 'está_amueblado': Un valor booleano que indica si la propiedad está amueblada.

44. 'está_cocina_equipada': Un valor booleano que indica si la propiedad tiene la cocina equipada.

45. 'es_accesible': Un valor booleano que indica si la propiedad es accesible para personas con discapacidad.

46. 'has_green_zones': Un valor booleano que indica si la propiedad cuenta con zonas verdes cercanas.

47. 'energy_certificate': Información sobre el certificado energético de la propiedad.

48. 'has_parking': Un valor booleano que indica si la propiedad tiene aparcamiento.

49. 'tiene_estacionamiento_privado': Un valor booleano que indica si la propiedad tiene estacionamiento privado.

50. 'tiene_estacionamiento_publico': Un valor booleano que indica si la propiedad tiene estacionamiento público cercano.

51. 'is_parking_included_in_price': Un valor booleano que indica si el aparcamiento está incluido en el precio de la propiedad.

52. 'parking_price': El precio del aparcamiento si no está incluido en el precio de la propiedad.

53. 'is_orientation_north': Un valor booleano que indica si la propiedad tiene orientación al norte.

54. 'es_orientación_oeste': Un valor booleano que indica si la propiedad tiene orientación al oeste.

55. 'es_orientación_sur': Un valor booleano que indica si la propiedad tiene orientación al sur.

56. 'es_orientación_este': Un valor booleano que indica si la propiedad tiene orientación al este.

***Cada una de estas características puede ser relevante para el análisis y predicción de precios o características de las propiedades inmobiliarias. Dependiendo de tus objetivos, deberás seleccionar las características más importantes y realizar el preprocesamiento adecuado para utilizarlas en tus modelos de machine learning.***
"""

pisos.info()

sns.distplot(pisos['buy_price'])

#Valores de simetría y curtosis antes de tratar
print("Skewness: %f" % pisos['buy_price'].skew())
print("Kurtosis: %f" % pisos['buy_price'].kurt())

"""***Podemos ver que la distribucion tiene un sesgo positivo***"""

Venta=pisos[pisos['is_buy_price_known']==True]
Venta=Venta['buy_price']
Venta.describe()

"""***Podemos observar una serie de detalles sobre los precios,el más económico
esta en 360000 euros mientras que el más caro en 8800000 euros, una ganga...***
"""

pisos.isnull().sum()

"""***Tenemos 179 datos duplicados y una serie de columnas sin datos que hay que eliminar,
son todas las que tienen 21742 datos nulos es decir, las ultimas 10 columnas, a la hora de eliminar
voy a eliminar también las columnas 'n_floors' y 'sq_mt_allotment', ya que tienen 20300 datos nulos cada una.
Después trataremos el resto de columnas ya que hay muchas en las que hay datos nulos y por lo tanto no son
uniformes y nos daran resultados sesgados***
"""

print(pisos.duplicated)

"""***Observo que con los datos duplicados no hay problema, sin embargo hay muchos valores nulos***"""

pisos.columns

print(pisos.shape)
pisos.head()

"""***Comienzo eliminando las columnas sin datos y las que entiendo que no voy a necesitar para la predicción,  entre ellas 'title', 'subtitle', 'operation' y 'rent_price', ya que me voy a centrar en el precio de venta de los pisos***"""

pisos.drop(columns=['title', 'subtitle', 'sq_mt_useful','n_floors',
       'sq_mt_allotment', 'latitude', 'longitude',
       'raw_address', 'is_exact_address_hidden', 'street_name',
       'street_number', 'portal', 'door',
       'rent_price_by_area', 'built_year',
       'is_rent_price_known', 'buy_price_by_area',
       'is_buy_price_known', 'is_furnished',
       'has_central_heating', 'rent_price',
       'are_pets_allowed',
       'is_kitchen_equipped',
       'energy_certificate', 'has_parking',
       'has_private_parking', 'has_public_parking',
       'is_parking_included_in_price', 'parking_price', 'is_orientation_north',
       'is_orientation_west', 'is_orientation_south', 'is_orientation_east'], inplace=True)

pisos.shape

pisos.describe().T

pisos.head()

"""***Vamos a obsevar la correlacion de los datos restantes respecto al precio de los pisos, veamos, ya
que si la correlacion es muy baja los datos no nos van a ayudar mucho, todo lo contrario***
"""

#corr = pisos.corr()
#corr
#Nose puede hacer la correlacion con datos que no sean numericos, primero limpia

"""***Hay tres datos que estan my correlacionados con el precio del piso, los metros, el numero de habitaciones y el numero de baños, evidentemente cuantos más metros, habitaciones o baños tenga una casa más cara será, ya que será de mayor tamaño, luego veremos como cuanto mayor sea la planta en caso de ser piso, tambien influye y por supuesto el distrito donde se vive, con las mismas caracteristicas es más caro un piso en el Barrio de Salamanca que otro que este en Vallecas.***"""

#pisos.corr()['buy_price'].sort_values(ascending=False)

"""***Hay varias columnas que no tienen ninguna correlación con el precio por lo que hay que eliminarlas tambien***"""

#Matriz de correlaciones
#corrmat = pisos.corr()
#top_corr_features = corrmat.index
#plt.figure(figsize=(20,20))
#g=sns.heatmap(pisos[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""***Visualización de la tendencia entre precios de compra y metros cuadrados***"""

# Visualización de la tendencia entre precios de compra y metros cuadrados
plt.figure(figsize=(12,8))
sns.scatterplot(x='sq_mt_built', y='buy_price', data=pisos)

"""***Visualización de la tendencia entre precios de compra y numero de habitaciones***"""

# Frecuencia por numero de habitaciones
plt.figure(figsize=(12,8))
sns.countplot(x='n_rooms', data=pisos)

pisos.sort_values('n_rooms', ascending=False).head(5)

"""***Visualización de la tendencia entre precios de compra y numero de baños***"""

# Frecuencia por numero de baños
plt.figure(figsize=(12,8))
sns.countplot(x='n_bathrooms', data=pisos)

pisos.sort_values('n_bathrooms', ascending=False).head(5)

"""***También esta correlacionado si la vivienda necesita restauración, si la necesita es más económico.***"""

# Frecuencia si necesita ser rehabilitada la vivienda.
plt.figure(figsize=(12,8))
sns.countplot(x='is_renewal_needed', data=pisos)

"""Precio más alto de piso en el Dataset"""

plt.figure(figsize=(15, 7))
sns.distplot(pisos["buy_price"], color="blue")
plt.xlabel("buy_price")
plt.title("Distribución de Precio de los pisos")
print("Precio más alto de piso en el Dataset: ", pisos["buy_price"].max())
plt.show()

"""Precio más bajo de piso en el Dataset"""

plt.figure(figsize=(15, 7))
sns.distplot(pisos["buy_price"], color="red")
plt.xlabel("buy_price")
plt.title("Distribución de Precio de los pisos")
print("Precio más bajo de piso en el Dataset: ", pisos["buy_price"].min())
plt.show()

#Limpeza de valores extremos
print("Antes de quitar extremos: ",pisos.shape)
index = pisos[(pisos['buy_price'] >= 1200000)|(pisos['buy_price'] <= 70000)].index
pisos.drop(index, inplace=True)
print("Despues de quitar extremos: " ,pisos.shape)

"""***Vamos a ver los tipos y numeros de valores que hay en cada columna.***"""

pisos.dtypes
for col in pisos:
    print(col)
    print(pisos[col].value_counts())
    print('')

"""***Vamos a ver como oscila el precio según la planta en la que esta situado el piso, cuanto mayor es la planta mayor es el precio,
los pisos de bajo, entresuelo y sótano son mas económicos.
Las casas y chalets aunque la mayoria estan en zonas bastante alejadas del centro de la ciudad son bastante caras, ya
que tiene muchos metros cuadrados.***
"""

floory = pisos["floor"]
buy = pisos["buy_price"]

#Distribución precio, planta
plt.figure(figsize= (30,15))
sns.boxplot(x= floory, y= buy, data= pisos, palette= sns.color_palette("cubehelix", 19),
            order=["1", "2", "3", "4", "Bajo","5", "6", "7", "8", "Entreplanta exterior","9", "Semi-sótano exterior", "Semi-sótano interior", "Entreplanta interior", "Sótano interior","Sótano","Sótano exterior","Entreplanta","Semi-sótano"])
plt.xlabel("Planta")
plt.ylabel("Precio")
plt.show()

"""***Podemos ver como cuanto mas alta es la planta mas caro es el piso, los de mayor precio estan en planta 9 y los sótanos
son los más economicos.

***Observando los valores de cada columna entiedo que hay varias columnas donde los valores NaN pueden ser perfectamente 'False', voy a ello.....***
"""

varios=['has_ac','has_fitted_wardrobes','has_garden','has_pool','has_terrace','has_balcony','has_storage_room','is_accessible','has_green_zones']
for col in varios:
    pisos[col]=pisos[col].fillna(False)
pisos.head()

"""***Según el tipo decasa podemos ver que los áticos, chalets y los duplex son mas caros que los pisos.***"""

pisos['house_type_id']=pisos['house_type_id'].fillna("Otro")

tipocasa = pisos['house_type_id']

tipocasa.unique()

#Distribución tipo de casa, precio
plt.figure(figsize= (30,15))
sns.boxplot(x= tipocasa, y= buy, data= pisos, palette= sns.color_palette("cubehelix", 19),
            order=["HouseType 1: Pisos", "HouseType 4: Dúplex", "Otro","HouseType 5: Áticos", "HouseType 2: Casa o chalet"])
plt.xlabel("Tipo de casa")
plt.ylabel("Precio")
plt.show()

"""***Vamos a crear una columna para cada tipo de casa para poder realizar mejor la predicción***"""

pisos=pd.get_dummies(pisos,columns=['house_type_id'])
#Es necesario borrar uno por lo que borraré le que he creado('Otro').
pisos.drop(columns='house_type_id_Otro', inplace=True)
pisos

"""**Ahora voy a proceder a eliminar las filas donde los metros y numero de baños son nulos, ya que tiene mucha correlación con el precio y tiene muy pocos valores nulos, unas 100 filas. ***"""

pisos=pisos[pisos['sq_mt_built'].notna()]
pisos=pisos[pisos['n_bathrooms'].notna()]

pisos.shape

"""***Ahora vamos a centrarnos en la zona donde estan situados los pisos y los vamos a clasificar por distritos***"""

pisos['district_id']=pisos['neighborhood_id'].copy()
pisos.district_id=pisos.district_id.str.extract(r'(District \d+)')
pisos.neighborhood_id=pisos.neighborhood_id.str.extract(r'(Neighborhood \d+)')
pisos.district_id=pisos.district_id.str.extract(r'(\d+)')
pisos.neighborhood_id=pisos.neighborhood_id.str.extract(r'(\d+)')
pisos

pisos.drop(columns='neighborhood_id', inplace=True)

pisos["district_id"].unique

"""***Por Distrito también encontramos diferencia en los precios de los inmuebles,el Distrito 11, Dsitrito 6, Distrito 5 y el Distrito 4
son los mas caros, mientras que el Distrito 12 y el Distrito 2 son bastante economicos.***
"""

plt.figure(figsize=(12,8))
sns.countplot(x='district_id', data=pisos)

distrito = pisos["district_id"]

#Distribución tipo de casa, precio
plt.figure(figsize= (30,15))
sns.boxplot(x= distrito, y= buy, data= pisos, palette= sns.color_palette("cubehelix", 20),
            order=["21", "19", "18","17", "14","13", "11", "12","", "10","8", "9", "6","7", "5","4", "3", "15","1", "2"])
plt.xlabel("Distrito")
plt.ylabel("Precio")
plt.show()

"""***En el caso de la calefaccion individual la correlacion es negativa ya que si tiene suele ser electrica y es menos
economico que tener calefacción central***
"""

calefaccion_individual = pisos['has_individual_heating']

#Distribución calefaccion_individual , precio
plt.figure(figsize= (30,15))
sns.boxplot(x= calefaccion_individual, y= buy, data= pisos, palette= sns.color_palette("cubehelix", 2),
            order=[True,False])
plt.xlabel("")
plt.ylabel("Precio")
plt.show()

pisos['has_individual_heating']=pisos['has_individual_heating'].fillna("Misc")

pisos=pd.get_dummies(pisos,columns=['has_individual_heating'])
#Tengo que eliminar uno asi que eliminaré el que he creado.
pisos.drop(columns='has_individual_heating_Misc', inplace=True)
pisos

pisos.head()

#pisos.corr()['buy_price'].sort_values(ascending=False)

pisos.drop(columns= ['operation','has_individual_heating_False','has_individual_heating_True'], inplace=True)

"""***Vamos a hacer numerica la columna de plantas para poder hcer la predicción."""

pisos['floor']=pisos['floor'].fillna("otro")

print(pisos['floor'].unique())

pisos["floor"]= pisos["floor"].map({"1":1, "2":2, "3":3, "4":4, "Bajo":0,"5":5, "6":6, "7":7, "8":8, "Entreplanta exterior":0.5,"9":9, "Semi-sótano exterior":-1, "Semi-sótano interior":-1, "Entreplanta interior":0.5, "Sótano interior":-2,"Sótano":-2,"Sótano exterior":-2,"Entreplanta":0.5,"Semi-sótano":-1,"otro":99})

pisos.head(20)

"""***Quiero observar las columnas que tenian solo el valor True y he sustituido por False todos los valores vacios***

***Son las siguientes: 'has_ac','has_fitted_wardrobes','has_garden','has_pool','has_terrace','has_balcony','has_storage_room','is_accessible','has_green_zones'.***
"""

columns_to_plot = ['has_ac','has_fitted_wardrobes','has_garden','has_pool','has_terrace','has_balcony','has_storage_room','is_accessible','has_green_zones']

# Configuración de los subplots
num_plots = len(columns_to_plot)
num_rows = 1
num_cols = num_plots
fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 5))

# Iterar sobre las columnas y crear los pie charts
for i, column in enumerate(columns_to_plot):
    # Obtener los datos de la columna
    data = pisos[column].value_counts()

    # Crear el gráfico de pie
    axs[i].pie(data, labels=data.index, autopct='%1.1f%%', startangle=90)
    axs[i].set_title(f'Pie Chart {column}')

# Ajuste de diseño
plt.tight_layout()

# Mostrar los gráficos
plt.show()

pisos.head()

pisos['has_lift']=pd.to_numeric(pisos['has_lift'],errors='coerce')

floor_under = pisos.groupby(['has_lift']).sum()


names = floor_under.index
values = floor_under['buy_price']
plt.title('TIENE ASCENSOR')
plt.pie(values,labels=names,autopct='%.2f%%')
plt.show()

pisos['is_exterior']=pd.to_numeric(pisos['is_exterior'],errors='coerce')

floor_under = pisos.groupby(['is_exterior']).sum()


names = floor_under.index
values = floor_under['buy_price']
plt.title('ES EXTERIOR')
plt.pie(values,labels=names,autopct='%.2f%%')
plt.show()

pisos['is_floor_under']=pisos['is_floor_under'].fillna("Misc")

pisos=pd.get_dummies(pisos,columns=['is_floor_under'])
#Need to drop one, so why not drop the one I created
pisos.drop(columns='is_floor_under_Misc', inplace=True)
pisos

pisos.dtypes

pisos.isnull().sum()

"""***VOY A RELLENAR LOS VALORES NULOS DE ESTAS COLUMNAS POR LA MODA, SON MUY POCOS DATOS LOS QUE HAY VACIOS.***"""

comode=pisos['is_new_development'].mode()[0]
pisos['is_new_development']=pisos['is_new_development'].fillna(comode)

marca=(pisos['has_lift'].isnull()) & (pisos['house_type_id_HouseType 2: Casa o chalet']==0)
lifmode=pisos['has_lift'].mode()[0]
pisos['has_lift'][marca]=lifmode
print(pisos['has_lift'].value_counts())

marca=(pisos['is_exterior'].isnull()) & (pisos['house_type_id_HouseType 2: Casa o chalet']==0)
exmode=pisos['is_exterior'].mode()[0]
pisos['is_exterior'][marca]=exmode
print(pisos['is_exterior'].value_counts())

flomode=pisos['floor'].mode()[0]
pisos['floor']=pisos['floor'].fillna(flomode)

total = pisos.isnull().sum()
total

pisos['has_lift']=pd.to_numeric(pisos['has_lift'],errors='coerce')
pisos['is_exterior']=pd.to_numeric(pisos['is_exterior'],errors='coerce')

X = pisos.drop(columns=['buy_price'])
X = X.astype('category')

# label encoder
from sklearn.preprocessing import LabelEncoder
X=X.apply(LabelEncoder().fit_transform)

X

y= pisos['buy_price']

X.head()

y

pisos.isnull().sum()

"""***VAMOS A VER EN GRÁFICO LAS COLUMNAS QUE TENEMOS***"""

pisos.hist(figsize = (18,16))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test =train_test_split(X, y, random_state=66, test_size=0.2)

import pickle
with open("columns_order.pkl", "wb") as f:
    pickle.dump(X_train.columns.tolist(), f)

# 3. (Opcional) Escalado y PCA si vas a usarlo
#    Pero si usas un modelo tipo XGBoost, probablemente no lo necesitas

"""# Escalo los datos

***El escalado de datos se refiere al proceso de transformar las variables numéricas de entrada para que estén en una escala similar.
El escalado de datos es esencialmente necesario por dos razones principales:
a. Para evitar el dominio de una característica sobre otras.
b. Para acelerar el proceso de convergencia.***
"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""# REALIZO LA PCA

***El PCA es una técnica de reducción de dimensionalidad que se utiliza para comprimir la información contenida en un conjunto de datos con muchas características en un conjunto de datos con menos características, conocidas como componentes principales. La idea detrás del PCA es transformar las variables originales en un nuevo conjunto de variables no correlacionadas llamadas componentes principales.
El PCA tiene varios propósitos importantes:
a. Reducción de la dimensionalidad mientras se retiene la mayor cantidad posible de información relevante.
b. Eliminación de la multicolinealidad (alta correlación).
c. Reducción del ruido  y mantener la información más importante.
d. Visualización de datos***
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)


"""***En resumen, el escalado de datos y el PCA son técnicas clave para mejorar la calidad de los datos y reducir la dimensionalidad en el análisis de machine learning. Estas prácticas ayudan a los modelos a converger más rápido, reducir el riesgo de dominio de una característica, eliminar la multicolinealidad y mejorar la interpretación de los datos.***"""

from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

"""# Lineal Regresión Multiple"""

lr_multiple = linear_model.LinearRegression()

lr_multiple.fit(X_train, y_train)

y_pred = lr_multiple.predict(X_test)
y_pred

y_test

print('DATOS DEL MODELO REGRESIÓN LINEAL MULTIPLE')
print()
print('Valor de las pendientes o coeficientes "a":')
print(lr_multiple.coef_)
print('Valor de la intersección o coeficiente "b":')
print(lr_multiple.intercept_)

print("Precisión del modelo:")
print(lr_multiple.score(X_train, y_train))

from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error

predas = lr_multiple.predict(X_test)
mae = mean_absolute_error(y_test, predas)
rmse = np.sqrt(mean_squared_error(y_test, predas))
print("MAE: %f"  % (mae))
print("RMSE: %f" % (rmse))
print(explained_variance_score(predas,y_test))

"""# XGB REGRESOR"""

from numpy import asarray
#import xgboost as xgb
from xgboost import XGBRegressor
model = XGBRegressor()
# fit model
model.fit(X,y)

solucion3=[model.score(X,y)]

solucion3

model.fit(X_train,y_train)
preds = model.predict(X_test)
mae = mean_absolute_error(y_test, preds)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("MAE: %f"  % (mae))
print("RMSE: %f" % (rmse))
print(explained_variance_score(preds,y_test))

#Implementación de algoritmo de árboles de decisión XGBoost, personalizando variables.
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error

data_dmatrix = xgb.DMatrix(data=X,label=y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)


xg_reg = xgb.XGBRegressor(objective ='reg:linear',
                          eta = 0.3,
                          min_child_weight = 1,
                          gamma = 0.1018506246381371,
                          colsample_bytree = 0.8629698417369874,
                          learning_rate = 0.06164827794908118,
                          max_depth = 5,
                          alpha = 8.072986901537691,
                          n_estimators = 127,
                          subsample= 0.6873761748867334)
xg_reg.fit(X_train,y_train)
preds = xg_reg.predict(X_test)
mae = mean_absolute_error(y_test, preds)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("MAE: %f"  % (mae))
print("RMSE: %f" % (rmse))
print(explained_variance_score(preds,y_test))

plt.figure(figsize=(12,8))

# Our predictions
plt.scatter(y_test, preds);

# Perfect predictions
plt.plot(y_test, y_test, color='r', label='perfect fit line')
plt.legend();

"""# ***EL QUE MEJOR RESULTADO DA ES EL XGB REGRESOR = 94,7%***

***PRIMERO VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60 METROS, 2 HABITACIONES, PLANTA 9 y EN EL DISTRITO MAS CARO, EL 5.***
"""

prueba11 = [60, 2, 1, 9, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 5, 0, 1]
new_data = asarray([prueba11])
# make a prediction
yhat = xg_reg.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""***LUEGO VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60 METROS, 2 HABITACIONES, PLANTA BAJA y EN EL DISTRITO MAS BARATO, EL 12.***"""

prueba22 = [60, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 12, 1, 0]
new_data = asarray([prueba22])
# make a prediction
yhat = xg_reg.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""***AHORA VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60 METROS, 2 HABITACIONES, PLANTA BAJA y EN EL DISTRITO MAS CARO, EL 5.***"""

prueba33 = [60, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 5, 0, 1]
new_data = asarray([prueba33])
# make a prediction
yhat = xg_reg.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""***FINALMENTE VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60 METROS, 2 HABITACIONES, PLANTA 9 y EN EL DISTRITO MAS BARATO, EL 12.***"""

prueba44 = [60, 2, 1, 9, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 12, 1, 0]
new_data = asarray([prueba44])
# make a prediction
yhat = xg_reg.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""# Support Vector Machine (SVM)"""

from sklearn.svm import SVR

svr = SVR(kernel="linear", C=1.0, epsilon=0.2)

svr.fit(X_train, y_train)

solucion_4=[svr.intercept_,svr.coef_,svr.score(X,y)]

solucion_4

print("DATOS DEL MODELO VECTORES DE SOPORTE REGRESIÓN")
print()
print("Precisión del modelo:")
print(svr.score(X_train, y_train))

predv = svr.predict(X_test)
mae = mean_absolute_error(y_test, predv)
rmse = np.sqrt(mean_squared_error(y_test, predv))
print("MAE: %f"  % (mae))
print("RMSE: %f" % (rmse))
print(explained_variance_score(predv,y_test))

"""# Decission Tree Regression"""

from sklearn.tree import DecisionTreeRegressor
adr = DecisionTreeRegressor(max_depth=5)

adr.fit(X_train, y_train)

y_pred = adr.predict(X_test)
y_pred

y_test

print("Datos del Modelo de árboles de decision Regresión")
print()
print("Precisión del modelo:")
print(adr.score(X_train, y_train))

predtree = adr.predict(X_test)
mae = mean_absolute_error(y_test, predtree)
rmse = np.sqrt(mean_squared_error(y_test, predtree))
print("MAE: %f"  % (mae))
print("RMSE: %f" % (rmse))
print(explained_variance_score(predtree,y_test))

"""# Regresión Líneal Polinomial"""

from sklearn.preprocessing import PolynomialFeatures

poli_reg = PolynomialFeatures(degree= 2)

X_train_poli = poli_reg.fit_transform(X_train)
X_test_poli = poli_reg.fit_transform(X_test)

pr = linear_model.LinearRegression()

pr.fit(X_train_poli, y_train)

y_pred = pr.predict(X_test_poli)
y_pred

y_test

print("Datos del modelo Regresion Lineal Polinomial")
print()
print("Precisión del modelo: ")
print(pr.score(X_train_poli, y_train))

"""# Random Forest Regression"""

from sklearn.ensemble import RandomForestRegressor
bar = RandomForestRegressor(n_estimators=300, max_depth=8)

bar.fit(X_train, y_train)

y_pred = bar.predict(X_test)
y_pred

y_test

#Implementación de algoritmo Bosques Aleatorios Regresión
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error

print("Datos del modelo Bosques Aleatorios Regresión")
print()
print("Precisión del modelo: ")
print(bar.score(X_train, y_train))

predis = bar.predict(X_test)
mae = mean_absolute_error(y_test, predis)
rmse = np.sqrt(mean_squared_error(y_test, predis))
print("MAE: %f"  % (mae))
print("RMSE: %f" % (rmse))
print(explained_variance_score(predis,y_test))

plt.figure(figsize=(12,8))

# Our predictions
plt.scatter(y_test, predis);

# Perfect predictions
plt.plot(y_test, y_test, color='r', label='perfect fit line')
plt.legend();

from sklearn.neighbors import KNeighborsRegressor

knn= KNeighborsRegressor(n_neighbors=2)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)
y_pred

y_test

print("Datos del modelo KNeighborsRegressor")
print()
print("Precisión del modelo: ")
print(knn.score(X_train, y_train))

KNeighborsRegressor(...)
print(knn.predict([[60, 2, 1, 9, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 12, 1, 0]]))

predoos = knn.predict(X_test)
mae = mean_absolute_error(y_test, predoos)
rmse = np.sqrt(mean_squared_error(y_test, predoos))
print("MAE: %f"  % (mae))
print("RMSE: %f" % (rmse))
print(explained_variance_score(predoos,y_test))

"""# **** VAMOS A OPTIMIZAR CON GridSearchCV****

# KNeighborsRegressor
"""

from sklearn.model_selection import GridSearchCV
params = [{
        'n_neighbors': [5,6,7,8],
    }]
kgrid = GridSearchCV(estimator=KNeighborsRegressor(),
                      param_grid=params,
                      cv=5,
                      verbose=0)
kgrid.fit(X_train, y_train)
print(kgrid.best_score_)
print(kgrid.best_estimator_)
print(kgrid.best_params_)
print(list(zip(kgrid.cv_results_['params'],kgrid.cv_results_['mean_test_score'])))

"""# XGBoost"""

from numpy import asarray
from numpy import mean
from numpy import std
from sklearn.datasets import make_regression
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from matplotlib import pyplot
# define dataset
X_train, y_train= make_regression(n_samples=1000, n_features=24, n_informative=5, random_state=1)
# evaluate the model
model = XGBRegressor(objective='reg:squarederror')
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')
print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

#XGBoost

params = {
    'max_depth': [10, 15, 20],
    'n_estimators': [50, 60, 70, 80],
    'learning_rate': [0.1]
}
xgrid = GridSearchCV(estimator=xgb.XGBRegressor(),
                      param_grid=params,
                      cv=10,
                      verbose=0)
xgrid.fit(X_train, y_train)
print(xgrid.best_score_)
print(xgrid.best_estimator_)
print(xgrid.best_params_)

#XGBoost
params = {
    'max_depth': [5,10,30,50],
    'n_estimators': [5,10,30,50],
    'learning_rate': [.5,0.1,0.05,0.01]
}
xgrid = GridSearchCV(estimator=xgb.XGBRegressor(),
                      param_grid=params,
                      cv=5,
                      verbose=0)
xgrid.fit(X_train, y_train)
print(xgrid.best_score_)
print(xgrid.best_estimator_)
print(xgrid.best_params_)

df=pd.DataFrame({
    'modelo':['Lineal Regresión Multiple','XGB REGRESOR','Support Vector Machine (SVM)','DecisionTreeRegressor','Regresión Líneal Polinomial', 'Bosques Aleatorios Regresión', 'KNeighborsRegressor' ,'KNeighborsRegressorGrid',
             'XGBoostGrid'],
    'score':[0.425, 0.947, 0.602, 0.783, 0.776, 0.892, 0.94, 0.802, 0.951]
})
df

"""# EL QUE DA MEJOR RESULTADO DA ES EL XGB REGRESOR.

***EJEMPLO --->***
***Vamos a realizar la prediccion con el algoritmo KNeighborsRegressor optimizado con GridSearchCV***

***PRIMERO VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60 M,2 HAB,1 BAÑO,PLANTA 9 Y EN EL DISTRITO MAS CARO, EL 5.***
"""

prueba11 = [60, 2, 1, 9, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 5, 0, 1]
new_data = asarray([prueba11])
# make a prediction
yhat = kgrid.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""***LUEGO VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60 M,2 HAB,1 BAÑO,PLANTA BAJA Y EN EL DISTRITO MAS BARATO, EL 12.***"""

prueba22 = [60, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 12, 1, 0]
new_data = asarray([prueba22])
# make a prediction
yhat = kgrid.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""***AHORA VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60 M,2 HAB,1 BAÑO PLANTA BAJA Y EN EL DISTRITO MAS CARO, EL 5.***"""

prueba33 = [60, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 5, 0, 1]
new_data = asarray([prueba33])
# make a prediction
yhat = kgrid.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""***FINALMENTE VAMOS A VER EL PRECIO DE UNA VIVIENDA DE 60M,2HAB,1 BAÑO,PLANTA 9 Y EN EL DISTRITO MAS BARATO, EL 12.***"""

prueba44 = [60, 2, 1, 9, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 12, 1, 0]
new_data = asarray([prueba44])
# make a prediction
yhat = kgrid.predict(new_data)
# summarize prediction
print('Precio de vivienda: %.2f euros' % yhat)

"""# ***Rosana Longares***"""

import pickle
from numpy import asarray
#import xgboost as xgb
from xgboost import XGBRegressor
model = XGBRegressor()
# fit model
model.fit(X,y)



# Guardar el modelo entrenado
nombre_archivo = 'modelo_entrenado.pkl'
with open(nombre_archivo, 'wb') as archivo:
    pickle.dump(model, archivo)

print(f"Modelo guardado en {nombre_archivo}")

